{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_hyper_search.ipynb","provenance":[],"authorship_tag":"ABX9TyMQppdypPbRDh8GRViTJ1PS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setting up"],"metadata":{"id":"6IkMLskYA3jA"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBNJOl-A52NR","executionInfo":{"status":"ok","timestamp":1654351741420,"user_tz":240,"elapsed":15709,"user":{"displayName":"Alan Jiahui Ma","userId":"07426490136012605744"}},"outputId":"3ed06b3d-6a9d-44c1-8245-a769b4637d6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Lab/Grain-U-Net\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","FOLDERNAME = 'Lab/Grain-U-Net'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n","sys.path.append('/content/drive/MyDrive/{}/src'.format(FOLDERNAME))\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2\n","%aimport -tensorflow\n","!pip install bayesian-optimization  \n","!pip install -q plantcv\n","!pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nSWj7qEu55so","executionInfo":{"status":"ok","timestamp":1654351764786,"user_tz":240,"elapsed":19342,"user":{"displayName":"Alan Jiahui Ma","userId":"07426490136012605744"}},"outputId":"1a4ce290-e7ce-4a64-dc86-ee234f83010f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bayesian-optimization\n","  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.6)\n","Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n","Building wheels for collected packages: bayesian-optimization\n","  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=1912253ff5b3104140a2a1987f38ecf199d58b5e231e7c4a3becb5ca679f3efd\n","  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n","Successfully built bayesian-optimization\n","Installing collected packages: bayesian-optimization\n","Successfully installed bayesian-optimization-1.2.0\n","\u001b[K     |████████████████████████████████| 285 kB 5.0 MB/s \n","\u001b[K     |████████████████████████████████| 40 kB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 837 kB 43.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 58.0 MB/s \n","\u001b[K     |████████████████████████████████| 140 kB 63.3 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 61.8 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\u001b[0m\n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/drive/My%20Drive/Lab/Grain-U-Net\n","Installing collected packages: src\n","  Running setup.py develop for src\n","Successfully installed src-0.0.0\n"]}]},{"cell_type":"markdown","source":["# Impoting Data"],"metadata":{"id":"bgJyLW8VBCQS"}},{"cell_type":"code","source":["from pathlib import Path\n","import numpy as np\n","\n","train_dir = Path('data/train_atarashii_256')\n","\n","input_names = list(train_dir.glob('**/image/*.png'))\n","label_names = list(train_dir.glob('**/label/*.png'))\n","\n","print(f\"Found {len(input_names)} samples and {len(label_names)} tracings\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-Zowl826Cb4","executionInfo":{"status":"ok","timestamp":1654351780791,"user_tz":240,"elapsed":13267,"user":{"displayName":"Alan Jiahui Ma","userId":"07426490136012605744"}},"outputId":"2011dcb4-b88a-41c4-cbe4-9b4bac9698cb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 675 samples and 675 tracings\n"]}]},{"cell_type":"code","source":["from src import GrainSequence\n","import random\n","\n","def createSequences(batch_size): \n","    validation_samples = len(input_names) // 10\n","\n","    random.Random(1337).shuffle(input_names)\n","    random.Random(1337).shuffle(label_names)\n","\n","    train_input = input_names[:-validation_samples]\n","    train_label = label_names[:-validation_samples]\n","    train_gen = GrainSequence(batch_size, (256,256), train_input, train_label)\n","\n","    valid_input = input_names[-validation_samples:]\n","    valid_label = label_names[-validation_samples:]\n","    valid_gen = GrainSequence(batch_size, (256,256), valid_input, valid_label)\n","\n","    print(f\"Training set size: {len(train_input)}, {len(train_gen)} batches\")\n","    print(f\"Validation set size: {len(valid_input)}, {len(valid_gen)} batches\")\n","\n","    return [train_gen, valid_gen]"],"metadata":{"id":"ZHGRn1nDA05R","executionInfo":{"status":"ok","timestamp":1654351789825,"user_tz":240,"elapsed":7370,"user":{"displayName":"Alan Jiahui Ma","userId":"07426490136012605744"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Creating the model"],"metadata":{"id":"CdwYiN7WBnI3"}},{"cell_type":"code","source":["import numpy as np \n","from keras.optimizers import *\n","from src import get_unet_hyper\n","\n","def fit_with(dropout_rate, lr, epochs, batch_size, verbose=1):\n","    epochs = int(epochs)\n","    batch_size = int(batch_size)\n","    model = get_unet_hyper(dropout_rate=dropout_rate)\n","\n","    l = createSequences(batch_size)\n","    train_gen = l[0]\n","    valid_gen = l[1]\n","\n","    opt = keras.optimizers.Adam(learning_rate = lr)\n","    model.compile(optimizer = opt, loss='binary_crossentropy', metrics = ['accuracy'])\n","\n","    model.fit(x=train_gen, steps_per_epoch=len(train_gen), epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\n","    #add in test set\n","    score=model.evaluate(valid_gen , steps=len(valid_gen), verbose=0)\n","\n","    print('Test loss: ', score[0])\n","    print('Test accuracy: ', score[1])\n","\n","    return score[1]"],"metadata":{"id":"M6vcOaNlRYU7","executionInfo":{"status":"ok","timestamp":1654351909382,"user_tz":240,"elapsed":89,"user":{"displayName":"Alan Jiahui Ma","userId":"07426490136012605744"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","from bayes_opt import BayesianOptimization\n","import functools\n","\n","#dropout_rate, lr, epochs, batch_size\n","pbounds = {'dropout_rate': (0.3, 0.8), \n","           'lr': (1e-4, 1e-3), \n","           'epochs': (25, 40), \n","           'batch_size': (1,16)\n","           }\n"],"metadata":{"id":"tBJpKm-xBJj_","executionInfo":{"status":"ok","timestamp":1654351911570,"user_tz":240,"elapsed":110,"user":{"displayName":"Alan Jiahui Ma","userId":"07426490136012605744"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["optimizer = BayesianOptimization(\n","    f=fit_with, \n","    pbounds=pbounds, \n","    verbose=2, \n","    random_state=1\n",")\n","\n","optimizer.maximize(init_points=4, n_iter=10)\n","\n","for i, res in enumerate(optimizer.res):\n","    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n","\n","print(optimizer.max)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hg79BeInMf2r","executionInfo":{"status":"ok","timestamp":1654373684579,"user_tz":240,"elapsed":21771750,"user":{"displayName":"Alan Jiahui Ma","userId":"07426490136012605744"}},"outputId":"9f6c1e40-9b25-4bca-e5db-7228265444e9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["|   iter    |  target   | batch_... | dropou... |  epochs   |    lr     |\n","-------------------------------------------------------------------------\n","Training set size: 608, 86 batches\n","Validation set size: 67, 9 batches\n","Epoch 1/25\n","86/86 [==============================] - 215s 2s/step - loss: 0.3825 - accuracy: 0.9512\n","Epoch 2/25\n","86/86 [==============================] - 44s 511ms/step - loss: 0.1977 - accuracy: 0.9516\n","Epoch 3/25\n","86/86 [==============================] - 44s 505ms/step - loss: 0.1944 - accuracy: 0.9516\n","Epoch 4/25\n","86/86 [==============================] - 44s 508ms/step - loss: 0.1944 - accuracy: 0.9516\n","Epoch 5/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 6/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 7/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1942 - accuracy: 0.9516\n","Epoch 8/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1944 - accuracy: 0.9516\n","Epoch 9/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 10/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1941 - accuracy: 0.9516\n","Epoch 11/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1945 - accuracy: 0.9516\n","Epoch 12/25\n","86/86 [==============================] - 44s 508ms/step - loss: 0.1939 - accuracy: 0.9516\n","Epoch 13/25\n","86/86 [==============================] - 44s 506ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 14/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 15/25\n","86/86 [==============================] - 44s 506ms/step - loss: 0.1937 - accuracy: 0.9516\n","Epoch 16/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 17/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 18/25\n","86/86 [==============================] - 44s 506ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 19/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 20/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1939 - accuracy: 0.9516\n","Epoch 21/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1937 - accuracy: 0.9516\n","Epoch 22/25\n","86/86 [==============================] - 44s 507ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 23/25\n","86/86 [==============================] - 44s 506ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 24/25\n","86/86 [==============================] - 44s 506ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 25/25\n","86/86 [==============================] - 44s 506ms/step - loss: 0.1935 - accuracy: 0.9516\n","Test loss:  0.19128289818763733\n","Test accuracy:  0.952248215675354\n","| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9522  \u001b[0m | \u001b[0m 7.255   \u001b[0m | \u001b[0m 0.6602  \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 0.000372\u001b[0m |\n","Training set size: 608, 202 batches\n","Validation set size: 67, 22 batches\n","Epoch 1/27\n","202/202 [==============================] - 56s 245ms/step - loss: 0.2154 - accuracy: 0.9516\n","Epoch 2/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1954 - accuracy: 0.9518\n","Epoch 3/27\n","202/202 [==============================] - 48s 239ms/step - loss: 0.1945 - accuracy: 0.9518\n","Epoch 4/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1938 - accuracy: 0.9518\n","Epoch 5/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1940 - accuracy: 0.9518\n","Epoch 6/27\n","202/202 [==============================] - 48s 239ms/step - loss: 0.1938 - accuracy: 0.9518\n","Epoch 7/27\n","202/202 [==============================] - 48s 239ms/step - loss: 0.1935 - accuracy: 0.9518\n","Epoch 8/27\n","202/202 [==============================] - 48s 239ms/step - loss: 0.1935 - accuracy: 0.9518\n","Epoch 9/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1937 - accuracy: 0.9518\n","Epoch 10/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1936 - accuracy: 0.9518\n","Epoch 11/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1937 - accuracy: 0.9518\n","Epoch 12/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1933 - accuracy: 0.9518\n","Epoch 13/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1933 - accuracy: 0.9518\n","Epoch 14/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1936 - accuracy: 0.9518\n","Epoch 15/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1932 - accuracy: 0.9518\n","Epoch 16/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1934 - accuracy: 0.9518\n","Epoch 17/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 18/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1933 - accuracy: 0.9518\n","Epoch 19/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1931 - accuracy: 0.9518\n","Epoch 20/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1932 - accuracy: 0.9518\n","Epoch 21/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1928 - accuracy: 0.9518\n","Epoch 22/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 23/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1928 - accuracy: 0.9518\n","Epoch 24/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 25/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1929 - accuracy: 0.9518\n","Epoch 26/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1928 - accuracy: 0.9518\n","Epoch 27/27\n","202/202 [==============================] - 48s 238ms/step - loss: 0.1928 - accuracy: 0.9518\n","Test loss:  0.1937004178762436\n","Test accuracy:  0.951073169708252\n","| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9511  \u001b[0m | \u001b[0m 3.201   \u001b[0m | \u001b[0m 0.3462  \u001b[0m | \u001b[0m 27.79   \u001b[0m | \u001b[0m 0.000411\u001b[0m |\n","Training set size: 608, 101 batches\n","Validation set size: 67, 11 batches\n","Epoch 1/31\n","101/101 [==============================] - 53s 444ms/step - loss: 1.4713 - accuracy: 0.9493\n","Epoch 2/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1974 - accuracy: 0.9519\n","Epoch 3/31\n","101/101 [==============================] - 44s 440ms/step - loss: 0.1937 - accuracy: 0.9519\n","Epoch 4/31\n","101/101 [==============================] - 44s 439ms/step - loss: 0.1932 - accuracy: 0.9519\n","Epoch 5/31\n","101/101 [==============================] - 44s 439ms/step - loss: 0.1930 - accuracy: 0.9519\n","Epoch 6/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1929 - accuracy: 0.9519\n","Epoch 7/31\n","101/101 [==============================] - 44s 439ms/step - loss: 0.1929 - accuracy: 0.9519\n","Epoch 8/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1927 - accuracy: 0.9519\n","Epoch 9/31\n","101/101 [==============================] - 44s 439ms/step - loss: 0.1934 - accuracy: 0.9519\n","Epoch 10/31\n","101/101 [==============================] - 44s 440ms/step - loss: 0.1929 - accuracy: 0.9519\n","Epoch 11/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1926 - accuracy: 0.9519\n","Epoch 12/31\n","101/101 [==============================] - 44s 439ms/step - loss: 0.1930 - accuracy: 0.9519\n","Epoch 13/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1931 - accuracy: 0.9519\n","Epoch 14/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1926 - accuracy: 0.9519\n","Epoch 15/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1928 - accuracy: 0.9519\n","Epoch 16/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1927 - accuracy: 0.9519\n","Epoch 17/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1927 - accuracy: 0.9519\n","Epoch 18/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1929 - accuracy: 0.9519\n","Epoch 19/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1934 - accuracy: 0.9519\n","Epoch 20/31\n","101/101 [==============================] - 44s 439ms/step - loss: 0.1927 - accuracy: 0.9519\n","Epoch 21/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1928 - accuracy: 0.9519\n","Epoch 22/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1927 - accuracy: 0.9519\n","Epoch 23/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1924 - accuracy: 0.9519\n","Epoch 24/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1925 - accuracy: 0.9519\n","Epoch 25/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1927 - accuracy: 0.9519\n","Epoch 26/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1924 - accuracy: 0.9519\n","Epoch 27/31\n","101/101 [==============================] - 44s 437ms/step - loss: 0.1926 - accuracy: 0.9519\n","Epoch 28/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1924 - accuracy: 0.9519\n","Epoch 29/31\n","101/101 [==============================] - 44s 439ms/step - loss: 0.1921 - accuracy: 0.9519\n","Epoch 30/31\n","101/101 [==============================] - 44s 438ms/step - loss: 0.1925 - accuracy: 0.9519\n","Epoch 31/31\n","101/101 [==============================] - 44s 439ms/step - loss: 0.1926 - accuracy: 0.9519\n","Test loss:  0.197609543800354\n","Test accuracy:  0.9502500891685486\n","| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9503  \u001b[0m | \u001b[0m 6.952   \u001b[0m | \u001b[0m 0.5694  \u001b[0m | \u001b[0m 31.29   \u001b[0m | \u001b[0m 0.000716\u001b[0m |\n","Training set size: 608, 152 batches\n","Validation set size: 67, 16 batches\n","Epoch 1/25\n","152/152 [==============================] - 53s 305ms/step - loss: 5.2961 - accuracy: 0.9490\n","Epoch 2/25\n","152/152 [==============================] - 46s 305ms/step - loss: 0.1955 - accuracy: 0.9517\n","Epoch 3/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1943 - accuracy: 0.9517\n","Epoch 4/25\n","152/152 [==============================] - 46s 305ms/step - loss: 0.1939 - accuracy: 0.9517\n","Epoch 5/25\n","152/152 [==============================] - 47s 306ms/step - loss: 0.1941 - accuracy: 0.9517\n","Epoch 6/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1939 - accuracy: 0.9517\n","Epoch 7/25\n","152/152 [==============================] - 46s 305ms/step - loss: 0.1937 - accuracy: 0.9517\n","Epoch 8/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1936 - accuracy: 0.9517\n","Epoch 9/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1935 - accuracy: 0.9517\n","Epoch 10/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1936 - accuracy: 0.9517\n","Epoch 11/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1934 - accuracy: 0.9517\n","Epoch 12/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1936 - accuracy: 0.9517\n","Epoch 13/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1934 - accuracy: 0.9517\n","Epoch 14/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1931 - accuracy: 0.9517\n","Epoch 15/25\n","152/152 [==============================] - 46s 305ms/step - loss: 0.1932 - accuracy: 0.9517\n","Epoch 16/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1933 - accuracy: 0.9517\n","Epoch 17/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1934 - accuracy: 0.9517\n","Epoch 18/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1930 - accuracy: 0.9517\n","Epoch 19/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1934 - accuracy: 0.9517\n","Epoch 20/25\n","152/152 [==============================] - 46s 305ms/step - loss: 0.1933 - accuracy: 0.9517\n","Epoch 21/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1930 - accuracy: 0.9517\n","Epoch 22/25\n","152/152 [==============================] - 46s 305ms/step - loss: 0.1930 - accuracy: 0.9517\n","Epoch 23/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1931 - accuracy: 0.9517\n","Epoch 24/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1930 - accuracy: 0.9517\n","Epoch 25/25\n","152/152 [==============================] - 46s 304ms/step - loss: 0.1930 - accuracy: 0.9517\n","Test loss:  0.19315092265605927\n","Test accuracy:  0.9516301155090332\n","| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9516  \u001b[0m | \u001b[0m 4.067   \u001b[0m | \u001b[0m 0.7391  \u001b[0m | \u001b[0m 25.41   \u001b[0m | \u001b[0m 0.000703\u001b[0m |\n","Training set size: 608, 55 batches\n","Validation set size: 67, 6 batches\n","Epoch 1/25\n","55/55 [==============================] - 52s 754ms/step - loss: 92.3856 - accuracy: 0.9510\n","Epoch 2/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.2032 - accuracy: 0.9516\n","Epoch 3/25\n","55/55 [==============================] - 41s 749ms/step - loss: 0.1954 - accuracy: 0.9516\n","Epoch 4/25\n","55/55 [==============================] - 41s 749ms/step - loss: 0.1950 - accuracy: 0.9516\n","Epoch 5/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1944 - accuracy: 0.9516\n","Epoch 6/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1943 - accuracy: 0.9516\n","Epoch 7/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1945 - accuracy: 0.9516\n","Epoch 8/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1946 - accuracy: 0.9516\n","Epoch 9/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1945 - accuracy: 0.9516\n","Epoch 10/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 11/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1942 - accuracy: 0.9516\n","Epoch 12/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 13/25\n","55/55 [==============================] - 41s 751ms/step - loss: 0.1943 - accuracy: 0.9516\n","Epoch 14/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1944 - accuracy: 0.9516\n","Epoch 15/25\n","55/55 [==============================] - 41s 749ms/step - loss: 0.1941 - accuracy: 0.9516\n","Epoch 16/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1946 - accuracy: 0.9516\n","Epoch 17/25\n","55/55 [==============================] - 41s 749ms/step - loss: 0.1939 - accuracy: 0.9516\n","Epoch 18/25\n","55/55 [==============================] - 41s 748ms/step - loss: 0.1937 - accuracy: 0.9516\n","Epoch 19/25\n","55/55 [==============================] - 41s 749ms/step - loss: 0.1937 - accuracy: 0.9516\n","Epoch 20/25\n","55/55 [==============================] - 41s 751ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 21/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 22/25\n","55/55 [==============================] - 41s 751ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 23/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 24/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1937 - accuracy: 0.9516\n","Epoch 25/25\n","55/55 [==============================] - 41s 750ms/step - loss: 0.1936 - accuracy: 0.9516\n","Test loss:  0.18964150547981262\n","Test accuracy:  0.9527916312217712\n","| \u001b[95m 5       \u001b[0m | \u001b[95m 0.9528  \u001b[0m | \u001b[95m 11.24   \u001b[0m | \u001b[95m 0.8     \u001b[0m | \u001b[95m 25.0    \u001b[0m | \u001b[95m 0.001   \u001b[0m |\n","Training set size: 608, 38 batches\n","Validation set size: 67, 4 batches\n","Epoch 1/25\n","38/38 [==============================] - 51s 982ms/step - loss: 0.7091 - accuracy: 0.9019\n","Epoch 2/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6907 - accuracy: 0.9518\n","Epoch 3/25\n","38/38 [==============================] - 37s 971ms/step - loss: 0.6890 - accuracy: 0.9518\n","Epoch 4/25\n","38/38 [==============================] - 37s 971ms/step - loss: 0.6873 - accuracy: 0.9518\n","Epoch 5/25\n","38/38 [==============================] - 37s 972ms/step - loss: 0.6856 - accuracy: 0.9518\n","Epoch 6/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6839 - accuracy: 0.9518\n","Epoch 7/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6822 - accuracy: 0.9518\n","Epoch 8/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6805 - accuracy: 0.9518\n","Epoch 9/25\n","38/38 [==============================] - 37s 974ms/step - loss: 0.6789 - accuracy: 0.9518\n","Epoch 10/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6772 - accuracy: 0.9518\n","Epoch 11/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6755 - accuracy: 0.9518\n","Epoch 12/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6739 - accuracy: 0.9518\n","Epoch 13/25\n","38/38 [==============================] - 37s 974ms/step - loss: 0.6722 - accuracy: 0.9518\n","Epoch 14/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6706 - accuracy: 0.9518\n","Epoch 15/25\n","38/38 [==============================] - 37s 974ms/step - loss: 0.6689 - accuracy: 0.9518\n","Epoch 16/25\n","38/38 [==============================] - 37s 972ms/step - loss: 0.6673 - accuracy: 0.9518\n","Epoch 17/25\n","38/38 [==============================] - 37s 972ms/step - loss: 0.6657 - accuracy: 0.9518\n","Epoch 18/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6640 - accuracy: 0.9518\n","Epoch 19/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6624 - accuracy: 0.9518\n","Epoch 20/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6608 - accuracy: 0.9518\n","Epoch 21/25\n","38/38 [==============================] - 37s 972ms/step - loss: 0.6592 - accuracy: 0.9518\n","Epoch 22/25\n","38/38 [==============================] - 37s 972ms/step - loss: 0.6576 - accuracy: 0.9518\n","Epoch 23/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6559 - accuracy: 0.9518\n","Epoch 24/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6543 - accuracy: 0.9518\n","Epoch 25/25\n","38/38 [==============================] - 37s 973ms/step - loss: 0.6528 - accuracy: 0.9518\n","Test loss:  0.6520752906799316\n","Test accuracy:  0.950345516204834\n","| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9503  \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Training set size: 608, 60 batches\n","Validation set size: 67, 6 batches\n","Epoch 1/26\n","60/60 [==============================] - 53s 713ms/step - loss: 0.2161 - accuracy: 0.9514\n","Epoch 2/26\n","60/60 [==============================] - 43s 710ms/step - loss: 0.1997 - accuracy: 0.9516\n","Epoch 3/26\n","60/60 [==============================] - 43s 711ms/step - loss: 0.2016 - accuracy: 0.9516\n","Epoch 4/26\n","60/60 [==============================] - 43s 711ms/step - loss: 0.2006 - accuracy: 0.9516\n","Epoch 5/26\n","60/60 [==============================] - 43s 710ms/step - loss: 0.1988 - accuracy: 0.9516\n","Epoch 6/26\n","60/60 [==============================] - 43s 712ms/step - loss: 0.1986 - accuracy: 0.9516\n","Epoch 7/26\n","60/60 [==============================] - 43s 712ms/step - loss: 0.1982 - accuracy: 0.9516\n","Epoch 8/26\n","60/60 [==============================] - 43s 712ms/step - loss: 0.1969 - accuracy: 0.9516\n","Epoch 9/26\n","60/60 [==============================] - 43s 713ms/step - loss: 0.1955 - accuracy: 0.9516\n","Epoch 10/26\n","60/60 [==============================] - 43s 712ms/step - loss: 0.1951 - accuracy: 0.9516\n","Epoch 11/26\n","60/60 [==============================] - 43s 713ms/step - loss: 0.1941 - accuracy: 0.9516\n","Epoch 12/26\n","60/60 [==============================] - 43s 713ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 13/26\n","60/60 [==============================] - 43s 713ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 14/26\n","60/60 [==============================] - 43s 713ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 15/26\n","60/60 [==============================] - 43s 714ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 16/26\n","60/60 [==============================] - 43s 714ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 17/26\n","60/60 [==============================] - 43s 714ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 18/26\n","60/60 [==============================] - 43s 713ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 19/26\n","60/60 [==============================] - 43s 712ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 20/26\n","60/60 [==============================] - 43s 713ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 21/26\n","60/60 [==============================] - 43s 714ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 22/26\n","60/60 [==============================] - 43s 712ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 23/26\n","60/60 [==============================] - 43s 714ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 24/26\n","60/60 [==============================] - 43s 714ms/step - loss: 0.1932 - accuracy: 0.9516\n","Epoch 25/26\n","60/60 [==============================] - 43s 714ms/step - loss: 0.1933 - accuracy: 0.9516\n","Epoch 26/26\n","60/60 [==============================] - 43s 713ms/step - loss: 0.1934 - accuracy: 0.9516\n","Test loss:  0.19259265065193176\n","Test accuracy:  0.9517984986305237\n","| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9518  \u001b[0m | \u001b[0m 10.08   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 26.81   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Training set size: 608, 67 batches\n","Validation set size: 67, 7 batches\n","Epoch 1/25\n","67/67 [==============================] - 52s 632ms/step - loss: 0.4088 - accuracy: 0.9381\n","Epoch 2/25\n","67/67 [==============================] - 42s 630ms/step - loss: 0.1982 - accuracy: 0.9516\n","Epoch 3/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1951 - accuracy: 0.9516\n","Epoch 4/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1944 - accuracy: 0.9516\n","Epoch 5/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1942 - accuracy: 0.9516\n","Epoch 6/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1947 - accuracy: 0.9516\n","Epoch 7/25\n","67/67 [==============================] - 42s 629ms/step - loss: 0.1960 - accuracy: 0.9516\n","Epoch 8/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1948 - accuracy: 0.9516\n","Epoch 9/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1943 - accuracy: 0.9516\n","Epoch 10/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1942 - accuracy: 0.9516\n","Epoch 11/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1939 - accuracy: 0.9516\n","Epoch 12/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1939 - accuracy: 0.9516\n","Epoch 13/25\n","67/67 [==============================] - 42s 629ms/step - loss: 0.1942 - accuracy: 0.9516\n","Epoch 14/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1943 - accuracy: 0.9516\n","Epoch 15/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 16/25\n","67/67 [==============================] - 42s 629ms/step - loss: 0.1941 - accuracy: 0.9516\n","Epoch 17/25\n","67/67 [==============================] - 42s 629ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 18/25\n","67/67 [==============================] - 42s 629ms/step - loss: 0.1941 - accuracy: 0.9516\n","Epoch 19/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1939 - accuracy: 0.9516\n","Epoch 20/25\n","67/67 [==============================] - 42s 629ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 21/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 22/25\n","67/67 [==============================] - 42s 629ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 23/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1939 - accuracy: 0.9516\n","Epoch 24/25\n","67/67 [==============================] - 42s 628ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 25/25\n","67/67 [==============================] - 42s 629ms/step - loss: 0.1933 - accuracy: 0.9516\n","Test loss:  0.18934565782546997\n","Test accuracy:  0.9526456594467163\n","| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9526  \u001b[0m | \u001b[0m 9.415   \u001b[0m | \u001b[0m 0.7859  \u001b[0m | \u001b[0m 25.06   \u001b[0m | \u001b[0m 0.000639\u001b[0m |\n","Training set size: 608, 38 batches\n","Validation set size: 67, 4 batches\n","Epoch 1/40\n","38/38 [==============================] - 38s 968ms/step - loss: 0.6777 - accuracy: 0.9058\n","Epoch 2/40\n","38/38 [==============================] - 37s 968ms/step - loss: 0.6691 - accuracy: 0.9518\n","Epoch 3/40\n","38/38 [==============================] - 37s 961ms/step - loss: 0.6527 - accuracy: 0.9518\n","Epoch 4/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.6370 - accuracy: 0.9518\n","Epoch 5/40\n","38/38 [==============================] - 37s 966ms/step - loss: 0.6217 - accuracy: 0.9518\n","Epoch 6/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.6070 - accuracy: 0.9518\n","Epoch 7/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.5927 - accuracy: 0.9518\n","Epoch 8/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.5790 - accuracy: 0.9518\n","Epoch 9/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.5656 - accuracy: 0.9518\n","Epoch 10/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.5528 - accuracy: 0.9518\n","Epoch 11/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.5403 - accuracy: 0.9518\n","Epoch 12/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.5283 - accuracy: 0.9518\n","Epoch 13/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.5167 - accuracy: 0.9518\n","Epoch 14/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.5054 - accuracy: 0.9518\n","Epoch 15/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.4946 - accuracy: 0.9518\n","Epoch 16/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.4841 - accuracy: 0.9518\n","Epoch 17/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.4740 - accuracy: 0.9518\n","Epoch 18/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.4642 - accuracy: 0.9518\n","Epoch 19/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.4548 - accuracy: 0.9518\n","Epoch 20/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.4457 - accuracy: 0.9518\n","Epoch 21/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.4369 - accuracy: 0.9518\n","Epoch 22/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.4284 - accuracy: 0.9518\n","Epoch 23/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.4201 - accuracy: 0.9518\n","Epoch 24/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.4122 - accuracy: 0.9518\n","Epoch 25/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.4046 - accuracy: 0.9518\n","Epoch 26/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.3972 - accuracy: 0.9518\n","Epoch 27/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.3900 - accuracy: 0.9518\n","Epoch 28/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.3831 - accuracy: 0.9518\n","Epoch 29/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.3765 - accuracy: 0.9518\n","Epoch 30/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.3701 - accuracy: 0.9518\n","Epoch 31/40\n","38/38 [==============================] - 37s 966ms/step - loss: 0.3639 - accuracy: 0.9518\n","Epoch 32/40\n","38/38 [==============================] - 37s 966ms/step - loss: 0.3579 - accuracy: 0.9518\n","Epoch 33/40\n","38/38 [==============================] - 37s 965ms/step - loss: 0.3521 - accuracy: 0.9518\n","Epoch 34/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.3465 - accuracy: 0.9518\n","Epoch 35/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.3411 - accuracy: 0.9518\n","Epoch 36/40\n","38/38 [==============================] - 37s 963ms/step - loss: 0.3359 - accuracy: 0.9518\n","Epoch 37/40\n","38/38 [==============================] - 37s 964ms/step - loss: 0.3309 - accuracy: 0.9518\n","Epoch 38/40\n","38/38 [==============================] - 37s 963ms/step - loss: 0.3260 - accuracy: 0.9518\n","Epoch 39/40\n","38/38 [==============================] - 37s 963ms/step - loss: 0.3213 - accuracy: 0.9518\n","Epoch 40/40\n","38/38 [==============================] - 37s 963ms/step - loss: 0.3168 - accuracy: 0.9518\n","Test loss:  0.3160146176815033\n","Test accuracy:  0.9505667686462402\n","| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9506  \u001b[0m | \u001b[0m 16.0    \u001b[0m | \u001b[0m 0.8     \u001b[0m | \u001b[0m 40.0    \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n","Training set size: 608, 608 batches\n","Validation set size: 67, 67 batches\n","Epoch 1/40\n","608/608 [==============================] - 66s 102ms/step - loss: 0.2052 - accuracy: 0.9515\n","Epoch 2/40\n","608/608 [==============================] - 62s 102ms/step - loss: 0.1983 - accuracy: 0.9516\n","Epoch 3/40\n","608/608 [==============================] - 62s 102ms/step - loss: 0.1951 - accuracy: 0.9516\n","Epoch 4/40\n","608/608 [==============================] - 62s 102ms/step - loss: 0.1943 - accuracy: 0.9516\n","Epoch 5/40\n","608/608 [==============================] - 62s 102ms/step - loss: 0.1942 - accuracy: 0.9516\n","Epoch 6/40\n","608/608 [==============================] - 62s 102ms/step - loss: 0.1941 - accuracy: 0.9516\n","Epoch 7/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 8/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 9/40\n","608/608 [==============================] - 62s 102ms/step - loss: 0.1937 - accuracy: 0.9516\n","Epoch 10/40\n","608/608 [==============================] - 62s 102ms/step - loss: 0.1937 - accuracy: 0.9516\n","Epoch 11/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 12/40\n","608/608 [==============================] - 62s 102ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 13/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 14/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1936 - accuracy: 0.9516\n","Epoch 15/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1933 - accuracy: 0.9516\n","Epoch 16/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 17/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 18/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1932 - accuracy: 0.9516\n","Epoch 19/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 20/40\n","608/608 [==============================] - 62s 101ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 21/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1930 - accuracy: 0.9516\n","Epoch 22/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1932 - accuracy: 0.9516\n","Epoch 23/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 24/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1930 - accuracy: 0.9516\n","Epoch 25/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1929 - accuracy: 0.9516\n","Epoch 26/40\n","608/608 [==============================] - 61s 100ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 27/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 28/40\n","608/608 [==============================] - 61s 100ms/step - loss: 0.1928 - accuracy: 0.9516\n","Epoch 29/40\n","608/608 [==============================] - 61s 100ms/step - loss: 0.1927 - accuracy: 0.9516\n","Epoch 30/40\n","608/608 [==============================] - 61s 100ms/step - loss: 0.1929 - accuracy: 0.9516\n","Epoch 31/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1927 - accuracy: 0.9516\n","Epoch 32/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1928 - accuracy: 0.9516\n","Epoch 33/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1927 - accuracy: 0.9516\n","Epoch 34/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1927 - accuracy: 0.9516\n","Epoch 35/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1927 - accuracy: 0.9516\n","Epoch 36/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1927 - accuracy: 0.9516\n","Epoch 37/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1927 - accuracy: 0.9516\n","Epoch 38/40\n","608/608 [==============================] - 61s 101ms/step - loss: 0.1926 - accuracy: 0.9516\n","Epoch 39/40\n","608/608 [==============================] - 61s 100ms/step - loss: 0.1927 - accuracy: 0.9516\n","Epoch 40/40\n","608/608 [==============================] - 61s 100ms/step - loss: 0.1927 - accuracy: 0.9516\n","Test loss:  0.1877191811800003\n","Test accuracy:  0.9528423547744751\n","| \u001b[95m 10      \u001b[0m | \u001b[95m 0.9528  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.8     \u001b[0m | \u001b[95m 40.0    \u001b[0m | \u001b[95m 0.0001  \u001b[0m |\n","Training set size: 608, 152 batches\n","Validation set size: 67, 16 batches\n","Epoch 1/40\n","152/152 [==============================] - 49s 313ms/step - loss: 0.2075 - accuracy: 0.9515\n","Epoch 2/40\n","152/152 [==============================] - 48s 312ms/step - loss: 0.2004 - accuracy: 0.9516\n","Epoch 3/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.2003 - accuracy: 0.9516\n","Epoch 4/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1985 - accuracy: 0.9516\n","Epoch 5/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1976 - accuracy: 0.9516\n","Epoch 6/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1959 - accuracy: 0.9516\n","Epoch 7/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1949 - accuracy: 0.9516\n","Epoch 8/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 9/40\n","152/152 [==============================] - 48s 312ms/step - loss: 0.1943 - accuracy: 0.9516\n","Epoch 10/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 11/40\n","152/152 [==============================] - 48s 312ms/step - loss: 0.1941 - accuracy: 0.9516\n","Epoch 12/40\n","152/152 [==============================] - 48s 312ms/step - loss: 0.1938 - accuracy: 0.9516\n","Epoch 13/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1940 - accuracy: 0.9516\n","Epoch 14/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1937 - accuracy: 0.9516\n","Epoch 15/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 16/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 17/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1933 - accuracy: 0.9516\n","Epoch 18/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 19/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 20/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 21/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1932 - accuracy: 0.9516\n","Epoch 22/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1935 - accuracy: 0.9516\n","Epoch 23/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 24/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 25/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1933 - accuracy: 0.9516\n","Epoch 26/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 27/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1934 - accuracy: 0.9516\n","Epoch 28/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1933 - accuracy: 0.9516\n","Epoch 29/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 30/40\n","152/152 [==============================] - 48s 314ms/step - loss: 0.1933 - accuracy: 0.9516\n","Epoch 31/40\n","152/152 [==============================] - 48s 314ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 32/40\n","152/152 [==============================] - 47s 311ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 33/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1929 - accuracy: 0.9516\n","Epoch 34/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1930 - accuracy: 0.9516\n","Epoch 35/40\n","152/152 [==============================] - 48s 312ms/step - loss: 0.1929 - accuracy: 0.9516\n","Epoch 36/40\n","152/152 [==============================] - 48s 313ms/step - loss: 0.1929 - accuracy: 0.9516\n","Epoch 37/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1930 - accuracy: 0.9516\n","Epoch 38/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1931 - accuracy: 0.9516\n","Epoch 39/40\n","152/152 [==============================] - 47s 312ms/step - loss: 0.1928 - accuracy: 0.9516\n","Epoch 40/40\n","152/152 [==============================] - 47s 311ms/step - loss: 0.1927 - accuracy: 0.9516\n","Test loss:  0.18849621713161469\n","Test accuracy:  0.9529762268066406\n","| \u001b[95m 11      \u001b[0m | \u001b[95m 0.953   \u001b[0m | \u001b[95m 4.856   \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 40.0    \u001b[0m | \u001b[95m 0.0001  \u001b[0m |\n","Training set size: 608, 608 batches\n","Validation set size: 67, 67 batches\n","Epoch 1/36\n","608/608 [==============================] - 62s 100ms/step - loss: 1.4706 - accuracy: 0.9515\n","Epoch 2/36\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1950 - accuracy: 0.9515\n","Epoch 3/36\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1948 - accuracy: 0.9515\n","Epoch 4/36\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1945 - accuracy: 0.9515\n","Epoch 5/36\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1942 - accuracy: 0.9515\n","Epoch 6/36\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 7/36\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 8/36\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 9/36\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 10/36\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1986 - accuracy: 0.9515\n","Epoch 11/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 12/36\n","608/608 [==============================] - 59s 98ms/step - loss: 0.1941 - accuracy: 0.9515\n","Epoch 13/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 14/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 15/36\n","608/608 [==============================] - 59s 98ms/step - loss: 0.1941 - accuracy: 0.9515\n","Epoch 16/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1946 - accuracy: 0.9515\n","Epoch 17/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 18/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 19/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1944 - accuracy: 0.9515\n","Epoch 20/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 21/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1942 - accuracy: 0.9515\n","Epoch 22/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1944 - accuracy: 0.9515\n","Epoch 23/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1941 - accuracy: 0.9515\n","Epoch 24/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1943 - accuracy: 0.9515\n","Epoch 25/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1941 - accuracy: 0.9515\n","Epoch 26/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1939 - accuracy: 0.9515\n","Epoch 27/36\n","608/608 [==============================] - 59s 98ms/step - loss: 0.1942 - accuracy: 0.9515\n","Epoch 28/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1940 - accuracy: 0.9515\n","Epoch 29/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1939 - accuracy: 0.9515\n","Epoch 30/36\n","608/608 [==============================] - 59s 98ms/step - loss: 0.1938 - accuracy: 0.9515\n","Epoch 31/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1938 - accuracy: 0.9515\n","Epoch 32/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1938 - accuracy: 0.9515\n","Epoch 33/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1938 - accuracy: 0.9515\n","Epoch 34/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1937 - accuracy: 0.9515\n","Epoch 35/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1937 - accuracy: 0.9515\n","Epoch 36/36\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1935 - accuracy: 0.9515\n","Test loss:  0.1889181137084961\n","Test accuracy:  0.9534358978271484\n","| \u001b[95m 12      \u001b[0m | \u001b[95m 0.9534  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 36.29   \u001b[0m | \u001b[95m 0.001   \u001b[0m |\n","Training set size: 608, 202 batches\n","Validation set size: 67, 22 batches\n","Epoch 1/37\n","202/202 [==============================] - 49s 236ms/step - loss: 0.6888 - accuracy: 0.9446\n","Epoch 2/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.6797 - accuracy: 0.9516\n","Epoch 3/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.6708 - accuracy: 0.9516\n","Epoch 4/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.6621 - accuracy: 0.9516\n","Epoch 5/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.6536 - accuracy: 0.9516\n","Epoch 6/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.6452 - accuracy: 0.9516\n","Epoch 7/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.6369 - accuracy: 0.9516\n","Epoch 8/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.6287 - accuracy: 0.9516\n","Epoch 9/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.6207 - accuracy: 0.9516\n","Epoch 10/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.6128 - accuracy: 0.9516\n","Epoch 11/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.6049 - accuracy: 0.9516\n","Epoch 12/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5973 - accuracy: 0.9516\n","Epoch 13/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5897 - accuracy: 0.9516\n","Epoch 14/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5822 - accuracy: 0.9516\n","Epoch 15/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.5748 - accuracy: 0.9516\n","Epoch 16/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.5676 - accuracy: 0.9516\n","Epoch 17/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5604 - accuracy: 0.9516\n","Epoch 18/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5533 - accuracy: 0.9516\n","Epoch 19/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5464 - accuracy: 0.9516\n","Epoch 20/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5395 - accuracy: 0.9516\n","Epoch 21/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5328 - accuracy: 0.9516\n","Epoch 22/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.5261 - accuracy: 0.9516\n","Epoch 23/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.5195 - accuracy: 0.9516\n","Epoch 24/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5130 - accuracy: 0.9516\n","Epoch 25/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.5066 - accuracy: 0.9516\n","Epoch 26/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.5003 - accuracy: 0.9516\n","Epoch 27/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.4941 - accuracy: 0.9516\n","Epoch 28/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.4880 - accuracy: 0.9516\n","Epoch 29/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.4820 - accuracy: 0.9516\n","Epoch 30/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.4760 - accuracy: 0.9516\n","Epoch 31/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.4702 - accuracy: 0.9516\n","Epoch 32/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.4644 - accuracy: 0.9516\n","Epoch 33/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.4587 - accuracy: 0.9516\n","Epoch 34/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.4531 - accuracy: 0.9516\n","Epoch 35/37\n","202/202 [==============================] - 47s 231ms/step - loss: 0.4476 - accuracy: 0.9516\n","Epoch 36/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.4421 - accuracy: 0.9516\n","Epoch 37/37\n","202/202 [==============================] - 47s 232ms/step - loss: 0.4368 - accuracy: 0.9516\n","Test loss:  0.4333649277687073\n","Test accuracy:  0.9526968002319336\n","| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9527  \u001b[0m | \u001b[0m 3.539   \u001b[0m | \u001b[0m 0.8     \u001b[0m | \u001b[0m 37.07   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Training set size: 608, 608 batches\n","Validation set size: 67, 67 batches\n","Epoch 1/33\n","608/608 [==============================] - 62s 100ms/step - loss: 13.4275 - accuracy: 0.9516\n","Epoch 2/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1942 - accuracy: 0.9518\n","Epoch 3/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1939 - accuracy: 0.9518\n","Epoch 4/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1939 - accuracy: 0.9518\n","Epoch 5/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1937 - accuracy: 0.9518\n","Epoch 6/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1939 - accuracy: 0.9518\n","Epoch 7/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1937 - accuracy: 0.9518\n","Epoch 8/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1937 - accuracy: 0.9518\n","Epoch 9/33\n","608/608 [==============================] - 61s 99ms/step - loss: 0.1935 - accuracy: 0.9518\n","Epoch 10/33\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1935 - accuracy: 0.9518\n","Epoch 11/33\n","608/608 [==============================] - 61s 99ms/step - loss: 0.1951 - accuracy: 0.9518\n","Epoch 12/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1935 - accuracy: 0.9518\n","Epoch 13/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1934 - accuracy: 0.9518\n","Epoch 14/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1931 - accuracy: 0.9518\n","Epoch 15/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1931 - accuracy: 0.9518\n","Epoch 16/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 17/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1933 - accuracy: 0.9518\n","Epoch 18/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1931 - accuracy: 0.9518\n","Epoch 19/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 20/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1931 - accuracy: 0.9518\n","Epoch 21/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.1931 - accuracy: 0.9518\n","Epoch 22/33\n","608/608 [==============================] - 60s 99ms/step - loss: 0.2086 - accuracy: 0.9518\n","Epoch 23/33\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1936 - accuracy: 0.9518\n","Epoch 24/33\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1932 - accuracy: 0.9518\n","Epoch 25/33\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1933 - accuracy: 0.9518\n","Epoch 26/33\n","608/608 [==============================] - 59s 98ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 27/33\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 28/33\n","608/608 [==============================] - 59s 98ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 29/33\n","608/608 [==============================] - 59s 98ms/step - loss: 0.1929 - accuracy: 0.9518\n","Epoch 30/33\n","608/608 [==============================] - 59s 97ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 31/33\n","608/608 [==============================] - 59s 98ms/step - loss: 0.1935 - accuracy: 0.9518\n","Epoch 32/33\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1930 - accuracy: 0.9518\n","Epoch 33/33\n","608/608 [==============================] - 60s 98ms/step - loss: 0.1930 - accuracy: 0.9518\n","Test loss:  0.1960655152797699\n","Test accuracy:  0.9508065581321716\n","| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9508  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.8     \u001b[0m | \u001b[0m 33.67   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n","=========================================================================\n","Iteration 0: \n","\t{'target': 0.952248215675354, 'params': {'batch_size': 7.25533007053861, 'dropout_rate': 0.660162246721079, 'epochs': 25.001715622260175, 'lr': 0.00037209931536865575}}\n","Iteration 1: \n","\t{'target': 0.951073169708252, 'params': {'batch_size': 3.2013383622566955, 'dropout_rate': 0.3461692973843989, 'epochs': 27.793903170665065, 'lr': 0.00041100465433874294}}\n","Iteration 2: \n","\t{'target': 0.9502500891685486, 'params': {'batch_size': 6.9515121134600495, 'dropout_rate': 0.5694083670016785, 'epochs': 31.28791771604942, 'lr': 0.0007166975503570836}}\n","Iteration 3: \n","\t{'target': 0.9516301155090332, 'params': {'batch_size': 4.066783745972762, 'dropout_rate': 0.7390587181954726, 'epochs': 25.410813897968893, 'lr': 0.0007034207591605621}}\n","Iteration 4: \n","\t{'target': 0.9527916312217712, 'params': {'batch_size': 11.24132977639206, 'dropout_rate': 0.8, 'epochs': 25.0, 'lr': 0.001}}\n","Iteration 5: \n","\t{'target': 0.950345516204834, 'params': {'batch_size': 16.0, 'dropout_rate': 0.3, 'epochs': 25.0, 'lr': 0.0001}}\n","Iteration 6: \n","\t{'target': 0.9517984986305237, 'params': {'batch_size': 10.076026003735969, 'dropout_rate': 0.3, 'epochs': 26.80911516601179, 'lr': 0.0001}}\n","Iteration 7: \n","\t{'target': 0.9526456594467163, 'params': {'batch_size': 9.415444822686286, 'dropout_rate': 0.7859390598181556, 'epochs': 25.056200041115847, 'lr': 0.0006395089742813435}}\n","Iteration 8: \n","\t{'target': 0.9505667686462402, 'params': {'batch_size': 16.0, 'dropout_rate': 0.8, 'epochs': 40.0, 'lr': 0.001}}\n","Iteration 9: \n","\t{'target': 0.9528423547744751, 'params': {'batch_size': 1.0, 'dropout_rate': 0.8, 'epochs': 40.0, 'lr': 0.0001}}\n","Iteration 10: \n","\t{'target': 0.9529762268066406, 'params': {'batch_size': 4.855876202609185, 'dropout_rate': 0.3, 'epochs': 40.0, 'lr': 0.0001}}\n","Iteration 11: \n","\t{'target': 0.9534358978271484, 'params': {'batch_size': 1.0, 'dropout_rate': 0.3, 'epochs': 36.28919429868652, 'lr': 0.001}}\n","Iteration 12: \n","\t{'target': 0.9526968002319336, 'params': {'batch_size': 3.539019032478933, 'dropout_rate': 0.8, 'epochs': 37.07321285653314, 'lr': 0.0001}}\n","Iteration 13: \n","\t{'target': 0.9508065581321716, 'params': {'batch_size': 1.0, 'dropout_rate': 0.8, 'epochs': 33.66638090068591, 'lr': 0.001}}\n","{'target': 0.9534358978271484, 'params': {'batch_size': 1.0, 'dropout_rate': 0.3, 'epochs': 36.28919429868652, 'lr': 0.001}}\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"wjsDVRL_NG_P"},"execution_count":null,"outputs":[]}]}